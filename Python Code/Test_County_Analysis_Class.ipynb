{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_County_Analysis_Class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "sVFT_Ql6xfWl",
        "outputId": "9ac0e925-5962-42b6-b792-158c2146eda4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nOther ideas:\\n- return county with largest/smallest value for a specified variable of interest\\n- more plotting/visualizing\\n- exploring/visualizing across years (2012/2017) - like side by side graphs grace had\\n\\nOther to do:\\n- Build in check for whether or not variables are in columns (for target_var and var_list)....maybe do some fuzzy matching to match user input to variable column\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "countyAnalysis class used for county level analysis of food environment dataset\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define a class for county level analysis\n",
        "class countyAnalysis:\n",
        "  # Decription of class and fields\n",
        "\n",
        "    def __init__(self, df_county, target_var = None, var_list = None):\n",
        "      '''\n",
        "      Class constructor\n",
        "      df_county: county level dataframe (pre-formatted and cleaned)\n",
        "      target_var: optional, specify variable of interest for future analysis\n",
        "      var_list: optional, specific list of variables you're interested in exploring (i.e. their relation to target)\n",
        "      '''\n",
        "      self.df_county = df_county.copy() # Original dataframe, copy so that changes within class doesn't change original\n",
        "      self.target_var = target_var\n",
        "      self.var_list = var_list\n",
        "    \n",
        "    def select_state(self, state):\n",
        "      # Filter dataframe to counties from a specfific state\n",
        "      self.df_county = self.df_county[self.df_county['State']==state]\n",
        "    \n",
        "    def calculate_na_summary(self):\n",
        "      # Calculate percent missing\n",
        "      self.pct_missing = self.df_county.isnull().sum() * 100 / len(self.df_county)\n",
        "      # Find variable with largest % missing\n",
        "      print('Missing values summary: ')\n",
        "      print('The 10 variables with the highest percent missing are: ') \n",
        "      print(self.pct_missing.nlargest(10))\n",
        "    \n",
        "    def remove_missing_threshold_cols(self, threshold):\n",
        "      print(f\"Removing columns with greater than {(threshold)*100}% missing\")\n",
        "      # Remove variables with more than \"theshold\"% missing\n",
        "      # Threshold in drop na is number of observations so multiply theshold % by number of rows\n",
        "      self.df_county.dropna(thresh=self.df_county.shape[0]*(1-threshold), axis=1, inplace=True)\n",
        "    \n",
        "    def remove_missing_threshold_rows(self, threshold):\n",
        "      print(f\"Removing rows (counties) with greater than {(threshold)*100}% missing\")\n",
        "      # Remove rows (counties) with more than \"theshold\"% missing\n",
        "      # Threshold in drop na is number of observations so multiply theshold % by number of columns\n",
        "      self.df_county.dropna(thresh=self.df_county.shape[1]*(1-threshold), axis=0, inplace=True)\n",
        "    \n",
        "    def calculation_correlations_with_variable(self, num_pos_corr = 10, num_neg_corr = 10, display_all = False):\n",
        "      '''\n",
        "      num_pos_corr: number of top postive correlations to display. Default 10\n",
        "      num_neg_corr: number of top negative correlations to display. Default 10   \n",
        "      display_all: (default false), but if true will display all correlations (not just top pos/neg)\n",
        "      '''\n",
        "      # todo: skip non-numeric columns\n",
        "      # todo: option to remove high correlations\n",
        "      \n",
        "      # If haven't defined variable of interest, prompt user to select\n",
        "      if self.target_var is None:\n",
        "        self.select_target_var()\n",
        "      if self.var_list is not None: # If provided a list of variables only calculte correlations for those\n",
        "        self.corrs_with_var = self.df_county[self.var_list].apply(lambda x: x.corr(self.df_county[self.target_var]))\n",
        "      else: # Otherwise use all other variables (but only numeric columns)\n",
        "        numeric_cols = self.df_county.select_dtypes(include=np.number).columns.tolist() # Select only numeric columns to calculate correlations with target\n",
        "        self.corrs_with_var = self.df_county[numeric_cols].drop([self.target_var],axis=1).apply(lambda x: x.corr(self.df_county[self.target_var]))\n",
        "      if not display_all:\n",
        "        # Display top correlations\n",
        "        print(f'Top {num_pos_corr} largest (positives) correlations with {self.target_var}: ')\n",
        "        print(self.corrs_with_var.nlargest(num_pos_corr))\n",
        "        print(f'Top {num_neg_corr} smallest (negative) correlations {self.target_var}: ')\n",
        "        print(self.corrs_with_var.nsmallest(num_neg_corr))\n",
        "      else:\n",
        "        print(f\"Top correlations with {self.target_var}:\")\n",
        "        print(self.corrs_with_var.sort_values())\n",
        "    \n",
        "    \n",
        "    def find_zero_variance_state_cols(self, drop = False):\n",
        "      # Calculate standard deviation for each variable within each state\n",
        "      # To identify columns that are really at a state level \n",
        "      # (i.e. all counties within the state have the same value)\n",
        "      state_stdev = self.df_county.groupby(['State']).std()\n",
        "    \n",
        "      self.state_zero_var_cols = []\n",
        "      for col in state_stdev.columns:\n",
        "        if state_stdev[col].sum() == 0:\n",
        "          self.state_zero_var_cols.append(col)\n",
        "    \n",
        "      # If desired, drop columns that are really state level\n",
        "      if drop == True:\n",
        "        self.df_county.drop(self.state_zero_var_cols, axis=1, inplace=True)\n",
        "    \n",
        "    \n",
        "    def find_most_recent_data(self, drop = False):\n",
        "      '''\n",
        "      Some of the columns in this dataset measure the same thing but for different years.\n",
        "      For example: 'LACCESS_POP10' and 'LACCESS_POP15'.\n",
        "      For some analysis we may be interested in both of these columns while for others \n",
        "      we may only want to consider the most recent year.\n",
        "      This function helps to find the columns that contain data for multiple years\n",
        "      and gives the option of dropping any that are not the most recent.\n",
        "      '''\n",
        "      column_stub_dict = {}\n",
        "    \n",
        "      # Create dictionary of column name stubs (column name minus last two elements)\n",
        "      # With list of the last two elements (i.e. years) that match each name stub\n",
        "      for i, col in enumerate(self.df_county.columns):\n",
        "        if f'{col[:-2]}' in column_stub_dict: # If we've already searched for columns that match this beginning\n",
        "          continue # Skip this column\n",
        "        column_stub_dict[f'{col[:-2]}'] = [col[-2:]] # Create dictionary row for this beginning\n",
        "        for j, x in enumerate(self.df_county.columns[i+1:]): # Search subsequent columns for match\n",
        "          if col[:-2] == x[:-2]: # If the elements are the same (other than the last two (years))\n",
        "            column_stub_dict[f'{col[:-2]}'].append(x[-2:]) # Add the years to the dictionary\n",
        "    \n",
        "      # Use column name stub dictionary to find latest (and oldest) datapoint for each\n",
        "      self.list_recent_cols = []\n",
        "      self.list_non_recent_cols = []\n",
        "      for stub in column_stub_dict:\n",
        "        if len(column_stub_dict[stub])>1: # if there were more than one column (year) for this stub\n",
        "          int_year_lst = [int(year) for year in column_stub_dict[stub]] # Create list of ints so we can check for max\n",
        "          for year in column_stub_dict[stub]: # Loop through each year\n",
        "            if int(year) == max(int_year_lst): # If it is the max add to list_recent_cols\n",
        "              self.list_recent_cols.append(f'{stub}{year}')\n",
        "            else: # Otherwise add to list of non-recent columns\n",
        "              self.list_non_recent_cols.append(f'{stub}{year}')\n",
        "    \n",
        "      # Keep only the most recent data points (if desired)\n",
        "      if drop == True:\n",
        "        self.df_county.drop(self.list_non_recent_cols, axis=1, inplace=True)\n",
        "    \n",
        "    \n",
        "    def select_variables_to_analyze(self, n):\n",
        "      '''\n",
        "      n: number of variables to select\n",
        "      '''\n",
        "      print(f\"Randomly selecting {n} variables for analysis...\")\n",
        "      all_vars = [var for var in self.df_county.columns if var not in [\"FIPS\",    \"State\",    \"County\"]]\n",
        "      self.var_list = random.sample(all_vars, n)\n",
        "      print(f\"Selected variables: {self.var_list}\")\n",
        "    \n",
        "    def select_target_var(self):\n",
        "        print(\"Please select a variable of interest: \")\n",
        "        self.target_var = input()\n",
        "        if self.target_var not in self.df_county.columns:\n",
        "          print(\"This variable is not in this dataset.\")\n",
        "          print(\"Will be using default (LACCESS_POP15) until a valid variable is chosen.\")\n",
        "          self.target_var = \"LACCESS_POP15\"\n",
        "    \n",
        "    def append_region(self, data_path):\n",
        "        # Read in region data\n",
        "        df_region = pd.read_csv(data_path + 'State and Region.csv')\n",
        "        # Join region data to df_county\n",
        "        self.df_county = pd.merge(self.df_county, df_region, how='inner', on = 'State')\n",
        "    \n",
        "\n",
        "    def average_by_category(self, by_col, new_var_list = None):\n",
        "        '''\n",
        "        Calculate average value of variables by another column\n",
        "        '''\n",
        "        if new_var_list is not None: # If provided a variable list for this function use it\n",
        "            self.average_by = self.df_county.groupby(by_col, as_index=False)[new_var_list].mean()\n",
        "        elif self.var_list is not None: # Otherwise use var_list for the class\n",
        "          self.average_by = self.df_county.groupby(by_col, as_index=False)[self.var_list].mean()\n",
        "        else: # Otherwise calculate for all variables\n",
        "          self.average_by = self.df_county.groupby(by_col, as_index=False).mean()\n",
        "\n",
        "        return self.average_by\n",
        "\n",
        "    def labeled_categorical_cols(self):\n",
        "        # Create column that labels 0/1 in METRO13 variable\n",
        "        self.df_county['Metro'] = np.where(self.df_county['METRO13']==0, \"Non-metro\", \"Metro\")\n",
        "        # Create column that labels 0/1 in PERPOV10 variable\n",
        "        self.df_county['Persistent_Poverty'] = np.where(self.df_county['PERPOV10'] == 1, \"Persistent-Poverty\", \"Other\")\n",
        "\n",
        "\n",
        "'''\n",
        "Other ideas:\n",
        "- return county with largest/smallest value for a specified variable of interest\n",
        "- more plotting/visualizing\n",
        "- exploring/visualizing across years (2012/2017) - like side by side graphs grace had\n",
        "\n",
        "Other to do:\n",
        "- Build in check for whether or not variables are in columns (for target_var and var_list)....maybe do some fuzzy matching to match user input to variable column\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas.testing as pd_testing # Using assert_frame_equal() for unit tests\n",
        "import unittest"
      ],
      "metadata": {
        "id": "M7dDhb6FxkT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing class that inherits from unittest\n",
        "'''\n",
        "    When asked for a variable of interest:\n",
        "    1. 2010_Census_Population\n",
        "    2. 2010_Census_Population\n",
        "    3. Census_Population\n",
        "    4. 2010_Census_Population\n",
        "    \n",
        "    will need to input data_path for your computer in test_append_region_merge and test_append_region_not_equal\n",
        "'''\n",
        "class countyAnalysis_Test(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        #create small data base for testing\n",
        "        df_county = pd.DataFrame({'State': ['CA', 'VA', 'VA', 'MA', 'NY', 'TX', 'TN'],\n",
        "                                 'County': ['Autauga', 'Baldwin', 'Orange', 'King George', 'Stafford', 'Los Angeles', 'Carroll'],\n",
        "                                 '2010_Census_Population': [1928344.0, 1928345.0, 2734.0, 58392.0, 182394.0, 39328.0, 3827.0],\n",
        "                                 'VLFOODSEC_12_14': [7.2, 7.2, 7.2, 7.2, 7.2, 7.2, 7.2],\n",
        "                                 'BERRY_ACRES07': [np.nan, 81.0, np.nan, 79.0, np.nan, 28.0, np.nan],\n",
        "                                 'variable1': [1.2, 2.6, np.nan, 3.6, 7.8, 9.1, 8.1],\n",
        "                                 'LACCESS_POP15': [1400000.0, 5434000.0, 320000.0, 1230000.0, np.nan, np.nan, 304000.0],\n",
        "                                 'AGRITRSM_OPS07': [10.0, 16.0, 32.0, 6.0, 8.0, 10.0, 5.0],\n",
        "                                 'AGRITRSM_OPS12': [7.1, 7.1, 7.1, 7.1, 7.1, 7.1, 7.1],\n",
        "                                 'AGRITRSM_RCT12': [123943.0, 23954.0, 239234.0, 1234593.0, 98734.0, 93456.0, 92347.0],\n",
        "                                 'BERRY_ACRES12': [12.2, 5.3, 7.8, 0.1, 39.4, 3.4, 10.2],\n",
        "                                 'VEG_ACRESPTH07': [34.0, 51.0, 11.0, 10.0, 67.0, 45.0, 91.0],\n",
        "                                 'VEG_ACRESPTH12': [1.0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n",
        "                                 'VEG_FARMS07': [np.nan, 2.0, 10.0, 5.0, 4.3, np.nan, np.nan],\n",
        "                                 'VEG_FARMS12': [234.0, 2358.0, 5734.0, 9485.0, 983.0, np.nan, np.nan],\n",
        "                                 'METRO13': [0, 1, 0, 1, 0, 1, 0],\n",
        "                                 'PERPOV10': [0, 1, 0, 1, 0 , 1, 0]})\n",
        "        self.analysis1 = countyAnalysis(df_county)\n",
        "        \n",
        "    def test_select_state(self):\n",
        "        #does it grab the right state\n",
        "        self.analysis1.select_state('VA')\n",
        "        #test using assert equal, state should be VA\n",
        "        self.assertEqual(self.analysis1.df_county['State'].iloc[0], 'VA')\n",
        "       \n",
        "    def test_select_state_not_equal(self):\n",
        "        #does it not grab a state if it is not selected\n",
        "        self.analysis1.select_state('VA')\n",
        "        #test using assert not equal, should return 'VA' not 'NJ'\n",
        "        self.assertNotEqual(self.analysis1.df_county['State'].iloc[0], 'NJ')\n",
        "    \n",
        "    def test_pct_missing(self):\n",
        "        #test if na percentage is calculated correctly\n",
        "        self.analysis1.calculate_na_summary()\n",
        "        #test using assert equal, should return 85.71428571428571\n",
        "        self.assertEqual(self.analysis1.pct_missing.nlargest(1).iloc[0], 85.71428571428571)\n",
        "    \n",
        "    def test_calculate_na_summary(self): \n",
        "        #test if 10 are called\n",
        "        self.analysis1.calculate_na_summary()\n",
        "        #test using assert equal, should have 6 items in the list\n",
        "        self.assertEqual(len(self.analysis1.pct_missing.nlargest(10)), 10)\n",
        "       \n",
        "    def test_remove_missing_threshold_cols(self):\n",
        "        #test if it removes columns with a threshold greater than a given number\n",
        "        self.analysis1.remove_missing_threshold_cols(0.8)\n",
        "        #test using assert equal, should only remove 1 column\n",
        "        self.assertEqual(len(self.analysis1.df_county.columns), 16)\n",
        "        \n",
        "    def test_remove_missing_threshold_cols_no_cols(self):\n",
        "        #test if the threshold is too high none will be dropped\n",
        "        self.analysis1.remove_missing_threshold_cols(0.9)\n",
        "        #test using assert equal, shouldn't remove any columns\n",
        "        self.assertEqual(len(self.analysis1.df_county.columns), 17)\n",
        "        \n",
        "    def test_remove_missing_threshold_rows(self):\n",
        "        #test if it removes rows with a threshold greater than a given number\n",
        "        self.analysis1.remove_missing_threshold_rows(0.25)\n",
        "        #test using assert equal, should only remove one row\n",
        "        self.assertEqual(len(self.analysis1.df_county), 7)\n",
        "        \n",
        "    def test_remove_missing_threshold_rows_no_rows(self):\n",
        "        #test if the threshold is too high none will be dropped\n",
        "        self.analysis1.remove_missing_threshold_rows(0.3)\n",
        "        #test using assert equal, shouldn't remove any rows\n",
        "        self.assertEqual(len(self.analysis1.df_county), 7)\n",
        "        \n",
        "    def test_target_variable(self):\n",
        "        #test if the target variable works\n",
        "        target_var = '2010_Census_Population'\n",
        "        self.analysis1.target_var = target_var\n",
        "        #test using assert equal, target_var should be 2010_Census_Population\n",
        "        self.assertEqual(self.analysis1.target_var, '2010_Census_Population')\n",
        "        \n",
        "    def test_calculation_correlation_with_variable(self):\n",
        "        #test if method is calculating correlations\n",
        "        self.analysis1.calculation_correlations_with_variable()\n",
        "        #test using assert equal, the first correlation should be 0.7243301990584146 when using 2010_Census_Population\n",
        "        self.assertEqual(self.analysis1.corrs_with_var.nlargest(1).iloc[0], 0.7243301990584146)\n",
        "        \n",
        "    def test_calculation_correlation_with_variable_length(self):\n",
        "        #test if method calculates all correlations\n",
        "        self.analysis1.calculation_correlations_with_variable()\n",
        "        #test using assert equal, using a numeric column, there are 12 other numeric columns, use 2010_Census_Population\n",
        "        self.assertEqual(len(self.analysis1.corrs_with_var), 14)\n",
        "        \n",
        "    def test_find_zero_variance_state_cols(self):\n",
        "        #test if find_zero_variance_state_cols adds cols to list\n",
        "        self.analysis1.find_zero_variance_state_cols()\n",
        "        #test using assert equal, VLFOODSEC_12_14 should be the first column to have zero variance\n",
        "        self.assertEqual(self.analysis1.state_zero_var_cols[0], 'VLFOODSEC_12_14')\n",
        "        \n",
        "    def test_find_zero_variance_state_cols_length(self):\n",
        "        #test if method addes multipl cols to list\n",
        "        self.analysis1.find_zero_variance_state_cols()\n",
        "        #test using assert equal, length of list should be 5\n",
        "        self.assertEqual(len(self.analysis1.state_zero_var_cols), 5)\n",
        "        \n",
        "    #had trouble with find_most_recent_data method\n",
        "    \n",
        "    def test_select_variables_to_analyze_list(self):\n",
        "        #test if method selects the right number of varialbes\n",
        "        self.analysis1.select_variables_to_analyze(3)\n",
        "        #test using assert equal, the list should have 3 variables\n",
        "        self.assertEqual(len(self.analysis1.var_list), 3)\n",
        "        \n",
        "    def test_select_variables_to_analyze_exclusions(self):\n",
        "        #make sure FIPS, State, and County aren't in the list\n",
        "        self.analysis1.select_variables_to_analyze(3)\n",
        "        #test using assert not in, FIPS, State, and County should not be in the list\n",
        "        self.assertNotIn('FIPS', self.analysis1.var_list)\n",
        "        self.assertNotIn('State', self.analysis1.var_list)\n",
        "        self.assertNotIn('County', self.analysis1.var_list)         \n",
        "    \n",
        "    def test_select_target_var_not_in_dataset(self):\n",
        "        #test if target_var is \"Census_Population\" if variable is not in dataset\n",
        "        self.analysis1.select_target_var()\n",
        "        #test using assert equal, default target_var should be LACCESS_POP15\n",
        "        self.assertEqual(self.analysis1.target_var, '2010_Census_Population')\n",
        "        \n",
        "    def test_select_target_var_in_dataset(self):\n",
        "        #test if target_var is correct\n",
        "        self.analysis1.select_target_var()\n",
        "        #test using assert equal, target_var should be 2010_Census_Population\n",
        "        self.assertEqual(self.analysis1.target_var, 'LACCESS_POP15')\n",
        "        \n",
        "    def test_append_region_merge(self):\n",
        "        #test if region columns was added\n",
        "        self.analysis1.append_region('')\n",
        "        #test using assert in, Region should be in the dataset\n",
        "        self.assertIn('Region', self.analysis1.df_county)\n",
        "        \n",
        "    def test_append_region_first(self):\n",
        "        #test if region is correctly added\n",
        "        self.analysis1.append_region('')\n",
        "        #test using assert equal, the first instance in Region should be 'West'\n",
        "        self.assertEqual(self.analysis1.df_county['Region'].iloc[0], 'West')\n",
        "        \n",
        "    def test_average_by_category_new_var_list(self):\n",
        "        #test if average works with a new_var_list\n",
        "        self.analysis1.average_by_category('variable1', new_var_list = ['AGRITRSM_OPS07', 'AGRITRSM_OPS12', 'AGRITRSM_RCT12'])\n",
        "        #test using assert in, 'AGRITRSM_OPS07' is in the new average_by data frame\n",
        "        self.assertIn('AGRITRSM_OPS07', self.analysis1.average_by)\n",
        "        \n",
        "    def test_average_by_category_no_new_var_list(self):\n",
        "        #test if method works with no new_var_list\n",
        "        self.analysis1.average_by_category('variable1') #might need to create new instance\n",
        "        #test using assert equal, length of average_by should be 6\n",
        "        self.assertEqual(len(self.analysis1.average_by), 6)\n",
        "        \n",
        "    def test_labeled_categorical_cols_metro(self):\n",
        "        #test if Metro column is added\n",
        "        self.analysis1.labeled_categorical_cols()\n",
        "        #test using assert equal, \n",
        "        self.assertEqual(self.analysis1.df_county['Metro'].iloc[0], 'Non-metro')\n",
        "        \n",
        "    def test_labeled_categorical_cols_perpov(self):\n",
        "        #test if 'Persistent_Poverty' column is added\n",
        "        self.analysis1.labeled_categorical_cols()\n",
        "        #test using assert equal,\n",
        "        self.assertEqual(self.analysis1.df_county['Persistent_Poverty'].iloc[0], 'Other')  \n",
        "        \n",
        "    def test_find_most_recent_data_new_data(self):\n",
        "        #test if 'BERRY_ACRES12' is in list_recent_cols\n",
        "        self.analysis1.find_most_recent_data()\n",
        "        #test using assert in and assert not in, 'BERRY_ACRES12' should be in list_recent_cols and not in list_non_recent_cols\n",
        "        self.assertIn('BERRY_ACRES12', self.analysis1.list_recent_cols)\n",
        "        self.assertNotIn('BERRY_ACRES12', self.analysis1.list_non_recent_cols)\n",
        "        \n",
        "    def test_find_most_recent_data_old_data(self):\n",
        "        #test if 'BERRY_ACRES07' is in list_recent_cols\n",
        "        self.analysis1.find_most_recent_data()\n",
        "        #test using assert in and assert not in, 'BERRY_ACRES07' should not be in list_recent_cols and in list_non_recent_cols\n",
        "        self.assertNotIn('BERRY_ACRES07', self.analysis1.list_recent_cols)\n",
        "        self.assertIn('BERRY_ACRES07', self.analysis1.list_non_recent_cols)\n",
        "      \n",
        "            \n",
        "    "
      ],
      "metadata": {
        "id": "m0Pk3hFRjEZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=[''],exit=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unNV12oNjG7D",
        "outputId": "dec5d914-a7c1-4754-b749-1e681f9d7564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EE..."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values summary: \n",
            "The 10 variables with the highest percent missing are: \n",
            "VEG_ACRESPTH12            85.714286\n",
            "BERRY_ACRES07             57.142857\n",
            "VEG_FARMS07               42.857143\n",
            "LACCESS_POP15             28.571429\n",
            "VEG_FARMS12               28.571429\n",
            "variable1                 14.285714\n",
            "State                      0.000000\n",
            "County                     0.000000\n",
            "2010_Census_Population     0.000000\n",
            "VLFOODSEC_12_14            0.000000\n",
            "dtype: float64\n",
            "Please select a variable of interest: \n",
            "2010_Census_Population\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:2551: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  c = cov(x, y, rowvar)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:2480: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 largest (positives) correlations with 2010_Census_Population: \n",
            "LACCESS_POP15      7.243302e-01\n",
            "BERRY_ACRES07      5.359893e-01\n",
            "METRO13            8.531247e-02\n",
            "PERPOV10           8.531247e-02\n",
            "AGRITRSM_OPS07     1.852800e-02\n",
            "VLFOODSEC_12_14   -1.177987e-16\n",
            "AGRITRSM_OPS12    -1.177987e-16\n",
            "VEG_ACRESPTH07    -2.515006e-02\n",
            "BERRY_ACRES12     -7.339985e-02\n",
            "AGRITRSM_RCT12    -3.176141e-01\n",
            "dtype: float64\n",
            "Top 10 smallest (negative) correlations 2010_Census_Population: \n",
            "variable1         -8.129570e-01\n",
            "VEG_FARMS07       -7.050619e-01\n",
            "VEG_FARMS12       -6.216922e-01\n",
            "AGRITRSM_RCT12    -3.176141e-01\n",
            "BERRY_ACRES12     -7.339985e-02\n",
            "VEG_ACRESPTH07    -2.515006e-02\n",
            "VLFOODSEC_12_14   -1.177987e-16\n",
            "AGRITRSM_OPS12    -1.177987e-16\n",
            "AGRITRSM_OPS07     1.852800e-02\n",
            "METRO13            8.531247e-02\n",
            "dtype: float64\n",
            "Please select a variable of interest: \n",
            "2010_Census_Population\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".............."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 largest (positives) correlations with 2010_Census_Population: \n",
            "LACCESS_POP15      7.243302e-01\n",
            "BERRY_ACRES07      5.359893e-01\n",
            "METRO13            8.531247e-02\n",
            "PERPOV10           8.531247e-02\n",
            "AGRITRSM_OPS07     1.852800e-02\n",
            "VLFOODSEC_12_14   -1.177987e-16\n",
            "AGRITRSM_OPS12    -1.177987e-16\n",
            "VEG_ACRESPTH07    -2.515006e-02\n",
            "BERRY_ACRES12     -7.339985e-02\n",
            "AGRITRSM_RCT12    -3.176141e-01\n",
            "dtype: float64\n",
            "Top 10 smallest (negative) correlations 2010_Census_Population: \n",
            "variable1         -8.129570e-01\n",
            "VEG_FARMS07       -7.050619e-01\n",
            "VEG_FARMS12       -6.216922e-01\n",
            "AGRITRSM_RCT12    -3.176141e-01\n",
            "BERRY_ACRES12     -7.339985e-02\n",
            "VEG_ACRESPTH07    -2.515006e-02\n",
            "VLFOODSEC_12_14   -1.177987e-16\n",
            "AGRITRSM_OPS12    -1.177987e-16\n",
            "AGRITRSM_OPS07     1.852800e-02\n",
            "METRO13            8.531247e-02\n",
            "dtype: float64\n",
            "Missing values summary: \n",
            "The 10 variables with the highest percent missing are: \n",
            "VEG_ACRESPTH12            85.714286\n",
            "BERRY_ACRES07             57.142857\n",
            "VEG_FARMS07               42.857143\n",
            "LACCESS_POP15             28.571429\n",
            "VEG_FARMS12               28.571429\n",
            "variable1                 14.285714\n",
            "State                      0.000000\n",
            "County                     0.000000\n",
            "2010_Census_Population     0.000000\n",
            "VLFOODSEC_12_14            0.000000\n",
            "dtype: float64\n",
            "Removing columns with greater than 80.0% missing\n",
            "Removing columns with greater than 90.0% missing\n",
            "Removing rows (counties) with greater than 25.0% missing\n",
            "Removing rows (counties) with greater than 30.0% missing\n",
            "Please select a variable of interest: \n",
            "Census_Population\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This variable is not in this dataset.\n",
            "Will be using default (LACCESS_POP15) until a valid variable is chosen.\n",
            "Please select a variable of interest: \n",
            "2010_Census_Population\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly selecting 3 variables for analysis...\n",
            "Selected variables: ['variable1', 'AGRITRSM_OPS07', 'VEG_FARMS12']\n",
            "Randomly selecting 3 variables for analysis...\n",
            "Selected variables: ['VEG_ACRESPTH07', 'PERPOV10', 'BERRY_ACRES12']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "======================================================================\n",
            "ERROR: test_append_region_first (__main__.countyAnalysis_Test)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-a25b7e0797a5>\", line 148, in test_append_region_first\n",
            "    self.analysis1.append_region('')\n",
            "  File \"<ipython-input-1-be46f0cb36dc>\", line 149, in append_region\n",
            "    df_region = pd.read_csv(data_path + 'State and Region.csv')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 688, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 454, in _read\n",
            "    parser = TextFileReader(fp_or_buf, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 948, in __init__\n",
            "    self._make_engine(self.engine)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 1180, in _make_engine\n",
            "    self._engine = CParserWrapper(self.f, **self.options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 2010, in __init__\n",
            "    self._reader = parsers.TextReader(src, **kwds)\n",
            "  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
            "  File \"pandas/_libs/parsers.pyx\", line 674, in pandas._libs.parsers.TextReader._setup_parser_source\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'State and Region.csv'\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_append_region_merge (__main__.countyAnalysis_Test)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-a25b7e0797a5>\", line 142, in test_append_region_merge\n",
            "    self.analysis1.append_region('')\n",
            "  File \"<ipython-input-1-be46f0cb36dc>\", line 149, in append_region\n",
            "    df_region = pd.read_csv(data_path + 'State and Region.csv')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 688, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 454, in _read\n",
            "    parser = TextFileReader(fp_or_buf, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 948, in __init__\n",
            "    self._make_engine(self.engine)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 1180, in _make_engine\n",
            "    self._engine = CParserWrapper(self.f, **self.options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 2010, in __init__\n",
            "    self._reader = parsers.TextReader(src, **kwds)\n",
            "  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
            "  File \"pandas/_libs/parsers.pyx\", line 674, in pandas._libs.parsers.TextReader._setup_parser_source\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'State and Region.csv'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 25 tests in 22.923s\n",
            "\n",
            "FAILED (errors=2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f64d09c9b50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}